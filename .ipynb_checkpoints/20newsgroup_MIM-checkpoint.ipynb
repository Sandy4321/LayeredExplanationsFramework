{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "from __future__ imports must occur at the beginning of the file (cell_name, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"cell_name\"\u001b[1;36m, line \u001b[1;32m10\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m from __future__ imports must occur at the beginning of the file\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import lime\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.ensemble\n",
    "import sklearn.metrics\n",
    "from scipy.sparse import csr_matrix\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories = ['alt.atheism', 'soc.religion.christian']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories)\n",
    "class_names = ['atheism', 'christian']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (newsgroups_train.keys())\n",
    "print (newsgroups_train['data'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(lowercase=False)\n",
    "train_vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
    "test_vectors = vectorizer.transform(newsgroups_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id = vectorizer.vocabulary_\n",
    "id_to_word = {v:k for k, v in word_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = sklearn.ensemble.RandomForestClassifier(n_estimators=500)\n",
    "rf.fit(train_vectors, newsgroups_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = rf.predict(test_vectors)\n",
    "sklearn.metrics.f1_score(newsgroups_test.target, pred, average='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-7fa2197c1460>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mlime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlime_text\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLimeTextExplainer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_pipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnewsgroups_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "from lime import lime_text\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "c = make_pipeline(vectorizer, rf)\n",
    "print(c.predict_proba([newsgroups_test.data[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for idx in [3, 84, 101, # christian\n",
    "         2, 11, # atheis\n",
    "         4, 83, 9]: # borderline\n",
    "    explainer = LimeTextExplainer(class_names=class_names)\n",
    "    exp = explainer.explain_instance(newsgroups_test.data[idx], c.predict_proba, num_features=10)\n",
    "    print('Document id: %d' % idx)\n",
    "    exp.show_in_notebook(text=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIM\n",
    "## 1) indicator function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "def mim(data, labels, number, distance_metric='euclidean', typ=\"quadratic\"):\n",
    "    def alpha(dist,typ):\n",
    "        if typ==\"linear\":\n",
    "            return dist;\n",
    "        elif typ==\"quadratic\":\n",
    "            return dist**2;\n",
    "        elif typ==\"exponential\":\n",
    "            return math.exp(dist);\n",
    "    inf_vec = np.zeros(data[number].size);\n",
    "    poi = data[number];\n",
    "    poi_label = labels[number];\n",
    "    distances = metrics.pairwise_distances(data,poi.reshape(1, -1),metric=distance_metric).ravel();\n",
    "    \n",
    "    for y in range(len(data)):\n",
    "        diff = data[y] - poi;\n",
    "        dist = distances[y];\n",
    "        if dist!=0:\n",
    "            if poi_label==labels[y]:\n",
    "                inf_vec+=diff*1.0/alpha(dist,typ)\n",
    "            else:\n",
    "                inf_vec-=diff*1.0/alpha(dist,typ)\n",
    "    return inf_vec, distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1) test with dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from lime.lime_text import IndexedString, TextDomainMapper\n",
    "from lime.explanation import Explanation\n",
    "\n",
    "class ModifiedExplanation(Explanation):\n",
    "    def as_list(self, label=1, **kwargs):\n",
    "        label_to_use = self.available_labels()[0]\n",
    "        ans = self.domain_mapper.map_exp_ids(self.local_exp[label_to_use], **kwargs)\n",
    "        return ans\n",
    "\n",
    "    def as_pyplot_figure(self, label=1, **kwargs):\n",
    "        import matplotlib.pyplot as plt\n",
    "        exp = self.as_list(label=label, **kwargs)\n",
    "        fig = plt.figure()\n",
    "        vals = [x[1] for x in exp]\n",
    "        names = [x[0] for x in exp]\n",
    "        vals.reverse()\n",
    "        names.reverse()\n",
    "        colors = ['green' if x > 0 else 'red' for x in vals]\n",
    "        pos = np.arange(len(exp)) + .5\n",
    "        plt.barh(pos, vals, align='center', color=colors)\n",
    "        plt.yticks(pos, names)\n",
    "        if self.mode == \"classification\":\n",
    "            label = self.available_labels()[0]\n",
    "            title = 'MIM explanation for class %s' % self.class_names[label]\n",
    "        else:\n",
    "            title = 'MIM explanation'\n",
    "        plt.title(title)\n",
    "        return fig\n",
    "    \n",
    "def explain_text_with_mim(mim, text_instance, tf_idf_instance, \n",
    "                          predict_proba, predict_label,\n",
    "                          word_to_id, num_features = 5, \n",
    "                          class_names = ['atheism', 'christian']):\n",
    "    \n",
    "    indexed_string = IndexedString(text_instance, bow=True, split_expression=vectorizer.build_analyzer()) #r'(?u)\\b\\w\\w+\\b') #   r'\\W+'\n",
    "    domain_mapper = TextDomainMapper(indexed_string)\n",
    "    ret_exp = ModifiedExplanation(domain_mapper=domain_mapper, class_names=class_names)\n",
    "    ret_exp.predict_proba = predict_proba\n",
    "    \n",
    "    used_tfidf_features = np.nonzero(tf_idf_instance)[1]\n",
    "    mim_used_features = mim[used_tfidf_features]\n",
    "    \n",
    "    def tfidf_to_indexedstring(indexed_string, used_tfidf_features, word_to_id):\n",
    "        num_words = indexed_string.num_words()\n",
    "        mapping = {}\n",
    "\n",
    "        for index in range(num_words):\n",
    "            word = indexed_string.word(index)\n",
    "            if (word not in word_to_id.keys()):\n",
    "                continue\n",
    "            tfidf_feature = word_to_id[word]\n",
    "            if tfidf_feature not in mapping.keys():\n",
    "                mapping[tfidf_feature] = index\n",
    "                \n",
    "        return [mapping[i] for i in used_tfidf_features]\n",
    "        \n",
    "    used_features = tfidf_to_indexedstring(indexed_string, used_tfidf_features, word_to_id)\n",
    "    \n",
    "    \n",
    "    feature2mim = sorted(zip(used_features, mim_used_features.tolist()),\n",
    "                                             key=lambda x: np.abs(x[1]), reverse = True)\n",
    "    \n",
    "    ret_exp.local_exp[predict_label] = feature2mim[:num_features]\n",
    "    return ret_exp, feature2mim\n",
    "\n",
    "for idx in [3, 84, 101, # christian\n",
    "         2, 11, # atheis\n",
    "         4, 83, 9]: # borderline\n",
    "    data = csr_matrix.copy(train_vectors).toarray()\n",
    "    labels = np.copy(newsgroups_train.target)\n",
    "    prediction = rf.predict(data)\n",
    "    data_flatten = data.reshape(data.shape[0], -1)\n",
    "\n",
    "    data_poi= csr_matrix.copy(test_vectors[idx:idx+1]).toarray()\n",
    "    data = np.concatenate((data, data_poi), axis=0)\n",
    "    mim_infl, _ = mim(data_flatten, prediction, -1)\n",
    "    predict_proba = rf.predict_proba(data[-1:])[0]\n",
    "    predict_label = rf.predict(data[-1:])[0]\n",
    "\n",
    "    exp, feature2mim = explain_text_with_mim(mim=mim_infl, text_instance=newsgroups_test.data[idx], \n",
    "                                            tf_idf_instance=test_vectors[idx],\n",
    "                                            predict_proba=predict_proba, predict_label=predict_label,\n",
    "                                            word_to_id=word_to_id, num_features=10)\n",
    "\n",
    "    print('Document id: %d' % idx)\n",
    "    exp.show_in_notebook(text=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2) test with local explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import check_random_state\n",
    "\n",
    "class MIMTextExplainer():\n",
    "    def __init__(self, x_bow_vector, classifier_fn, random_state=None):\n",
    "        self.x_bow_vector = x_bow_vector\n",
    "        self.classifier_fn = classifier_fn\n",
    "        self.random_state = check_random_state(random_state)\n",
    "        \n",
    "    def explain_instance(self, num_samples=5000):\n",
    "        data, labels = self.generate_data_labels(self.classifier_fn, num_samples)\n",
    "        data_flatten = data.reshape(data.shape[0], -1)\n",
    "        mim_infl, _ = self.mim(data_flatten, labels, 0)\n",
    "        return mim_infl\n",
    "        \n",
    "    def generate_data_labels(self, classifier_fn, num_samples):  \n",
    "        x_vector = csr_matrix.copy(self.x_bow_vector).toarray()\n",
    "        x_idx_list = np.nonzero(x_vector)[1]\n",
    "        doc_size = len(x_idx_list)\n",
    "\n",
    "        sample = self.random_state.randint(1, doc_size+1, num_samples-1)\n",
    "        data = np.repeat(x_vector, repeats=num_samples, axis=0)\n",
    "        features_range = range(doc_size)\n",
    "        for i, size in enumerate(sample, start=1):\n",
    "            inactive = self.random_state.choice(features_range, size, replace=False)\n",
    "            inactive = [x_idx_list[i] for i in inactive]\n",
    "            data[i, inactive] = 0\n",
    "\n",
    "        labels = classifier_fn(data).flatten()\n",
    "        return data, labels \n",
    "    \n",
    "    def mim(self, data, labels, number, distance_metric='euclidean', typ=\"quadratic\"):\n",
    "        def alpha(dist,typ):\n",
    "            if typ==\"linear\":\n",
    "                return dist;\n",
    "            elif typ==\"quadratic\":\n",
    "                return dist**2;\n",
    "            elif typ==\"exponential\":\n",
    "                return math.exp(dist);\n",
    "        inf_vec = np.zeros(data[number].size);\n",
    "        poi = data[number];\n",
    "        poi_label = labels[number];\n",
    "        distances = metrics.pairwise_distances(data,poi.reshape(1, -1),metric=distance_metric).ravel();\n",
    "\n",
    "        for y in range(len(data)):\n",
    "            diff = data[y] - poi;\n",
    "            dist = distances[y];\n",
    "            if dist!=0:\n",
    "                if poi_label==labels[y]:\n",
    "                    inf_vec+=diff*1.0/alpha(dist,typ)\n",
    "                else:\n",
    "                    inf_vec-=diff*1.0/alpha(dist,typ)\n",
    "        return inf_vec, distances\n",
    "    \n",
    "for idx in [3, 84, 101, # christian\n",
    "         2, 11, # atheis\n",
    "         4, 83, 9]: # borderline\n",
    "\n",
    "    data = csr_matrix.copy(train_vectors).toarray()\n",
    "    labels = np.copy(newsgroups_train.target)\n",
    "    prediction = rf.predict(data)\n",
    "    \n",
    "    data_poi= csr_matrix.copy(test_vectors[idx:idx+1]).toarray()\n",
    "    data = np.concatenate((data, data_poi), axis=0)\n",
    "\n",
    "    explainer = MIMTextExplainer(test_vectors[idx:idx+1], rf.predict)\n",
    "    mim_infl = explainer.explain_instance()\n",
    "    predict_proba = rf.predict_proba(data[-1:])[0]\n",
    "    predict_label = rf.predict(data[-1:])[0]\n",
    "\n",
    "    exp, feature2mim = explain_text_with_mim(mim=mim_infl, text_instance=newsgroups_test.data[idx], \n",
    "                                            tf_idf_instance=test_vectors[idx],\n",
    "                                            predict_proba=predict_proba, predict_label=predict_label,\n",
    "                                            word_to_id=word_to_id, num_features=10)\n",
    "\n",
    "    print('Document id: %d' % idx)\n",
    "    exp.show_in_notebook(text=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3) test with local explanation - positive evidence only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import check_random_state\n",
    "\n",
    "class MIMTextExplainer():\n",
    "    def __init__(self, x_bow_vector, classifier_fn, random_state=None):\n",
    "        self.x_bow_vector = x_bow_vector\n",
    "        self.classifier_fn = classifier_fn\n",
    "        self.random_state = check_random_state(random_state)\n",
    "        \n",
    "    def explain_instance(self, num_samples=5000):\n",
    "        data, labels = self.generate_data_labels(self.classifier_fn, num_samples)\n",
    "        data_flatten = data.reshape(data.shape[0], -1)\n",
    "        mim_infl, _ = self.mim(data_flatten, labels, 0)\n",
    "        return mim_infl\n",
    "        \n",
    "    def generate_data_labels(self, classifier_fn, num_samples):  \n",
    "        x_vector = csr_matrix.copy(self.x_bow_vector).toarray()\n",
    "        x_idx_list = np.nonzero(x_vector)[1]\n",
    "        doc_size = len(x_idx_list)\n",
    "\n",
    "        sample = self.random_state.randint(1, doc_size+1, num_samples-1)\n",
    "        data = np.repeat(x_vector, repeats=num_samples, axis=0)\n",
    "        features_range = range(doc_size)\n",
    "        for i, size in enumerate(sample, start=1):\n",
    "            inactive = self.random_state.choice(features_range, size, replace=False)\n",
    "            inactive = [x_idx_list[i] for i in inactive]\n",
    "            data[i, inactive] = 0\n",
    "\n",
    "        labels = classifier_fn(data).flatten()\n",
    "        correct_label = labels[0]\n",
    "        selected_idx = (labels == correct_label) \n",
    "        data = data[selected_idx]\n",
    "        labels = labels[selected_idx]\n",
    "        print ('Ratio of positive evidences in sampling:{} / {}'.format(np.count_nonzero(selected_idx), selected_idx.size))\n",
    "        return data, labels \n",
    "    \n",
    "    def mim(self, data, labels, number, distance_metric='euclidean', typ=\"quadratic\"):\n",
    "        def alpha(dist,typ):\n",
    "            if typ==\"linear\":\n",
    "                return dist;\n",
    "            elif typ==\"quadratic\":\n",
    "                return dist**2;\n",
    "            elif typ==\"exponential\":\n",
    "                return math.exp(dist);\n",
    "        inf_vec = np.zeros(data[number].size);\n",
    "        poi = data[number];\n",
    "        poi_label = labels[number];\n",
    "        distances = metrics.pairwise_distances(data,poi.reshape(1, -1),metric=distance_metric).ravel();\n",
    "\n",
    "        for y in range(len(data)):\n",
    "            diff = data[y] - poi;\n",
    "            dist = distances[y];\n",
    "            if dist!=0:\n",
    "                if poi_label==labels[y]:\n",
    "                    inf_vec+=diff*1.0/alpha(dist,typ)\n",
    "                else:\n",
    "                    inf_vec-=diff*1.0/alpha(dist,typ)\n",
    "        return inf_vec, distances\n",
    "    \n",
    "for idx in [3, 84, 101, # christian\n",
    "         2, 11, # atheis\n",
    "         4, 83, 9]: # borderline\n",
    "\n",
    "    data = csr_matrix.copy(train_vectors).toarray()\n",
    "    labels = np.copy(newsgroups_train.target)\n",
    "    prediction = rf.predict(data)\n",
    "    \n",
    "    data_poi= csr_matrix.copy(test_vectors[idx:idx+1]).toarray()\n",
    "    data = np.concatenate((data, data_poi), axis=0)\n",
    "\n",
    "    explainer = MIMTextExplainer(test_vectors[idx:idx+1], rf.predict)\n",
    "    mim_infl = explainer.explain_instance()\n",
    "    predict_proba = rf.predict_proba(data[-1:])[0]\n",
    "    predict_label = rf.predict(data[-1:])[0]\n",
    "\n",
    "    exp, feature2mim = explain_text_with_mim(mim=mim_infl, text_instance=newsgroups_test.data[idx], \n",
    "                                            tf_idf_instance=test_vectors[idx],\n",
    "                                            predict_proba=predict_proba, predict_label=predict_label,\n",
    "                                            word_to_id=word_to_id, num_features=10)\n",
    "\n",
    "    print('Document id: %d' % idx)\n",
    "    exp.show_in_notebook(text=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) regression function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "def mim(data, labels, probabilities, number, distance_metric='euclidean', typ=\"quadratic\"):\n",
    "    def alpha(dist,typ):\n",
    "        if typ==\"linear\":\n",
    "            return dist;\n",
    "        elif typ==\"quadratic\":\n",
    "            return dist**2;\n",
    "        elif typ==\"exponential\":\n",
    "            return math.exp(dist);\n",
    "    inf_vec = np.zeros(data[number].size);\n",
    "    poi = data[number];\n",
    "    poi_label = labels[number];\n",
    "    distances = metrics.pairwise_distances(data,poi.reshape(1, -1),metric=distance_metric).ravel();\n",
    "    \n",
    "    for y in range(len(data)):\n",
    "        diff = data[y] - poi;\n",
    "        dist = distances[y];\n",
    "        if dist!=0:\n",
    "            poi_prob = probabilities[number][poi_label]\n",
    "            y_prob = probabilities[y][poi_label]\n",
    "            inf_vec+=diff*1.0/alpha(dist,typ)*(y_prob - poi_prob) \n",
    " \n",
    "    return inf_vec, distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1) test with dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from lime.lime_text import IndexedString, TextDomainMapper\n",
    "from lime.explanation import Explanation\n",
    "\n",
    "class ModifiedExplanation(Explanation):\n",
    "    def as_list(self, label=1, **kwargs):\n",
    "        label_to_use = self.available_labels()[0]\n",
    "        ans = self.domain_mapper.map_exp_ids(self.local_exp[label_to_use], **kwargs)\n",
    "        return ans\n",
    "\n",
    "    def as_pyplot_figure(self, label=1, **kwargs):\n",
    "        import matplotlib.pyplot as plt\n",
    "        exp = self.as_list(label=label, **kwargs)\n",
    "        fig = plt.figure()\n",
    "        vals = [x[1] for x in exp]\n",
    "        names = [x[0] for x in exp]\n",
    "        vals.reverse()\n",
    "        names.reverse()\n",
    "        colors = ['green' if x > 0 else 'red' for x in vals]\n",
    "        pos = np.arange(len(exp)) + .5\n",
    "        plt.barh(pos, vals, align='center', color=colors)\n",
    "        plt.yticks(pos, names)\n",
    "        if self.mode == \"classification\":\n",
    "            label = self.available_labels()[0]\n",
    "            title = 'MIM explanation for class %s' % self.class_names[label]\n",
    "        else:\n",
    "            title = 'MIM explanation'\n",
    "        plt.title(title)\n",
    "        return fig\n",
    "    \n",
    "def explain_text_with_mim(mim, text_instance, tf_idf_instance, \n",
    "                          predict_proba, predict_label,\n",
    "                          word_to_id, num_features = 5, \n",
    "                          class_names = ['atheism', 'christian']):\n",
    "    \n",
    "    indexed_string = IndexedString(text_instance, bow=True, split_expression=vectorizer.build_analyzer()) #r'(?u)\\b\\w\\w+\\b') #   r'\\W+'\n",
    "    domain_mapper = TextDomainMapper(indexed_string)\n",
    "    ret_exp = ModifiedExplanation(domain_mapper=domain_mapper, class_names=class_names)\n",
    "    ret_exp.predict_proba = predict_proba\n",
    "    \n",
    "    used_tfidf_features = np.nonzero(tf_idf_instance)[1]\n",
    "    mim_used_features = mim[used_tfidf_features]\n",
    "    \n",
    "    def tfidf_to_indexedstring(indexed_string, used_tfidf_features, word_to_id):\n",
    "        num_words = indexed_string.num_words()\n",
    "        mapping = {}\n",
    "\n",
    "        for index in range(num_words):\n",
    "            word = indexed_string.word(index)\n",
    "            if (word not in word_to_id.keys()):\n",
    "                continue\n",
    "            tfidf_feature = word_to_id[word]\n",
    "            if tfidf_feature not in mapping.keys():\n",
    "                mapping[tfidf_feature] = index\n",
    "                \n",
    "        return [mapping[i] for i in used_tfidf_features]\n",
    "        \n",
    "    used_features = tfidf_to_indexedstring(indexed_string, used_tfidf_features, word_to_id)\n",
    "    \n",
    "    \n",
    "    feature2mim = sorted(zip(used_features, mim_used_features.tolist()),\n",
    "                                             key=lambda x: np.abs(x[1]), reverse = True)\n",
    "    \n",
    "    ret_exp.local_exp[predict_label] = feature2mim[:num_features]\n",
    "    return ret_exp, feature2mim\n",
    "\n",
    "for idx in [3, 84, 101, # christian\n",
    "         2, 11, # atheis\n",
    "         4, 83, 9]: # borderline\n",
    "    data = csr_matrix.copy(train_vectors).toarray()\n",
    "    labels = np.copy(newsgroups_train.target)\n",
    "    prediction = rf.predict(data)\n",
    "    probabilities = rf.predict_proba(data)\n",
    "    data_flatten = data.reshape(data.shape[0], -1)\n",
    "\n",
    "    data_poi= csr_matrix.copy(test_vectors[idx:idx+1]).toarray()\n",
    "    data = np.concatenate((data, data_poi), axis=0)\n",
    "    mim_infl, _ = mim(data_flatten, prediction, probabilities, -1)\n",
    "    predict_proba = rf.predict_proba(data[-1:])[0]\n",
    "    predict_label = rf.predict(data[-1:])[0]\n",
    "\n",
    "    exp, feature2mim = explain_text_with_mim(mim=mim_infl, text_instance=newsgroups_test.data[idx], \n",
    "                                            tf_idf_instance=test_vectors[idx],\n",
    "                                            predict_proba=predict_proba, predict_label=predict_label,\n",
    "                                            word_to_id=word_to_id, num_features=10)\n",
    "\n",
    "    print('Document id: %d' % idx)\n",
    "    exp.show_in_notebook(text=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) test with local explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import check_random_state\n",
    "\n",
    "class MIMTextExplainer():\n",
    "    def __init__(self, x_bow_vector, classifier_fn, classifier_fn_proba, random_state=None):\n",
    "        self.x_bow_vector = x_bow_vector\n",
    "        self.classifier_fn = classifier_fn\n",
    "        self.classifier_fn_proba = classifier_fn_proba\n",
    "        self.random_state = check_random_state(random_state)\n",
    "        \n",
    "    def explain_instance(self, num_samples=5000):\n",
    "        data, labels = self.generate_data_labels(self.classifier_fn, num_samples)\n",
    "        probabilities = self.classifier_fn_proba(data)\n",
    "        data_flatten = data.reshape(data.shape[0], -1)\n",
    "        mim_infl, _ = self.mim(data_flatten, labels, probabilities, 0)\n",
    "        return mim_infl\n",
    "        \n",
    "    def generate_data_labels(self, classifier_fn, num_samples):  \n",
    "        x_vector = csr_matrix.copy(self.x_bow_vector).toarray()\n",
    "        x_idx_list = np.nonzero(x_vector)[1]\n",
    "        doc_size = len(x_idx_list)\n",
    "\n",
    "        sample = self.random_state.randint(1, doc_size+1, num_samples-1)\n",
    "        data = np.repeat(x_vector, repeats=num_samples, axis=0)\n",
    "        features_range = range(doc_size)\n",
    "        for i, size in enumerate(sample, start=1):\n",
    "            inactive = self.random_state.choice(features_range, size, replace=False)\n",
    "            inactive = [x_idx_list[i] for i in inactive]\n",
    "            data[i, inactive] = 0\n",
    "\n",
    "        labels = classifier_fn(data).flatten()\n",
    "        correct_label = labels[0]\n",
    "        selected_idx = (labels == correct_label) \n",
    "        return data, labels \n",
    "    \n",
    "    def mim(self, data, labels, probabilities, number, distance_metric='euclidean', typ=\"quadratic\"):\n",
    "        def alpha(dist,typ):\n",
    "            if typ==\"linear\":\n",
    "                return dist;\n",
    "            elif typ==\"quadratic\":\n",
    "                return dist**2;\n",
    "            elif typ==\"exponential\":\n",
    "                return math.exp(dist);\n",
    "        inf_vec = np.zeros(data[number].size);\n",
    "        poi = data[number];\n",
    "        poi_label = labels[number];\n",
    "        distances = metrics.pairwise_distances(data,poi.reshape(1, -1),metric=distance_metric).ravel();\n",
    "\n",
    "        for y in range(len(data)):\n",
    "            diff = data[y] - poi;\n",
    "            dist = distances[y];\n",
    "            if dist!=0:\n",
    "                poi_prob = probabilities[number][poi_label]\n",
    "                y_prob = probabilities[y][poi_label]\n",
    "                inf_vec+=diff*1.0/alpha(dist,typ)*(y_prob - poi_prob) \n",
    "\n",
    "        return inf_vec, distances\n",
    "    \n",
    "for idx in [3, 84, 101, # christian\n",
    "         2, 11, # atheis\n",
    "         4, 83, 9]: # borderline\n",
    "    \n",
    "    \n",
    "    data = csr_matrix.copy(train_vectors).toarray()\n",
    "    labels = np.copy(newsgroups_train.target)\n",
    "    prediction = rf.predict(data)\n",
    "    \n",
    "    data_poi= csr_matrix.copy(test_vectors[idx:idx+1]).toarray()\n",
    "    data = np.concatenate((data, data_poi), axis=0)\n",
    "    \n",
    "    explainer = MIMTextExplainer(test_vectors[idx:idx+1], rf.predict, rf.predict_proba)\n",
    "    mim_infl = explainer.explain_instance()\n",
    "    predict_proba = rf.predict_proba(data[-1:])[0]\n",
    "    predict_label = rf.predict(data[-1:])[0]\n",
    "\n",
    "    exp, feature2mim = explain_text_with_mim(mim=mim_infl, text_instance=newsgroups_test.data[idx], \n",
    "                                            tf_idf_instance=test_vectors[idx],\n",
    "                                            predict_proba=predict_proba, predict_label=predict_label,\n",
    "                                            word_to_id=word_to_id, num_features=10)\n",
    "\n",
    "    print('Document id: %d' % idx)\n",
    "    exp.show_in_notebook(text=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3) test with local explanation - positive evidence only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import check_random_state\n",
    "\n",
    "class MIMTextExplainer():\n",
    "    def __init__(self, x_bow_vector, classifier_fn, classifier_fn_proba, random_state=None):\n",
    "        self.x_bow_vector = x_bow_vector\n",
    "        self.classifier_fn = classifier_fn\n",
    "        self.classifier_fn_proba = classifier_fn_proba\n",
    "        self.random_state = check_random_state(random_state)\n",
    "        \n",
    "    def explain_instance(self, num_samples=5000):\n",
    "        data, labels = self.generate_data_labels(self.classifier_fn, num_samples)\n",
    "        probabilities = self.classifier_fn_proba(data)\n",
    "        data_flatten = data.reshape(data.shape[0], -1)\n",
    "        mim_infl, _ = self.mim(data_flatten, labels, probabilities, 0)\n",
    "        return mim_infl\n",
    "        \n",
    "    def generate_data_labels(self, classifier_fn, num_samples):  \n",
    "        x_vector = csr_matrix.copy(self.x_bow_vector).toarray()\n",
    "        x_idx_list = np.nonzero(x_vector)[1]\n",
    "        doc_size = len(x_idx_list)\n",
    "\n",
    "        sample = self.random_state.randint(1, doc_size+1, num_samples-1)\n",
    "        data = np.repeat(x_vector, repeats=num_samples, axis=0)\n",
    "        features_range = range(doc_size)\n",
    "        for i, size in enumerate(sample, start=1):\n",
    "            inactive = self.random_state.choice(features_range, size, replace=False)\n",
    "            inactive = [x_idx_list[i] for i in inactive]\n",
    "            data[i, inactive] = 0\n",
    "\n",
    "        labels = classifier_fn(data).flatten()\n",
    "        correct_label = labels[0]\n",
    "        selected_idx = (labels == correct_label) \n",
    "        data = data[selected_idx]\n",
    "        labels = labels[selected_idx]\n",
    "        print ('Ratio of positive evidences in sampling:{} / {}'.format(np.count_nonzero(selected_idx), selected_idx.size))\n",
    "        return data, labels \n",
    "    \n",
    "    def mim(self, data, labels, probabilities, number, distance_metric='euclidean', typ=\"quadratic\"):\n",
    "        def alpha(dist,typ):\n",
    "            if typ==\"linear\":\n",
    "                return dist;\n",
    "            elif typ==\"quadratic\":\n",
    "                return dist**2;\n",
    "            elif typ==\"exponential\":\n",
    "                return math.exp(dist);\n",
    "        inf_vec = np.zeros(data[number].size);\n",
    "        poi = data[number];\n",
    "        poi_label = labels[number];\n",
    "        distances = metrics.pairwise_distances(data,poi.reshape(1, -1),metric=distance_metric).ravel();\n",
    "\n",
    "        for y in range(len(data)):\n",
    "            diff = data[y] - poi;\n",
    "            dist = distances[y];\n",
    "            if dist!=0:\n",
    "                poi_prob = probabilities[number][poi_label]\n",
    "                y_prob = probabilities[y][poi_label]\n",
    "                inf_vec+=diff*1.0/alpha(dist,typ)*(y_prob - poi_prob) \n",
    "\n",
    "        return inf_vec, distances\n",
    "    \n",
    "for idx in [3, 84, 101, # christian\n",
    "         2, 11, # atheis\n",
    "         4, 83, 9]: # borderline\n",
    "    \n",
    "    \n",
    "    data = csr_matrix.copy(train_vectors).toarray()\n",
    "    labels = np.copy(newsgroups_train.target)\n",
    "    prediction = rf.predict(data)\n",
    "    \n",
    "    data_poi= csr_matrix.copy(test_vectors[idx:idx+1]).toarray()\n",
    "    data = np.concatenate((data, data_poi), axis=0)\n",
    "    \n",
    "    explainer = MIMTextExplainer(test_vectors[idx:idx+1], rf.predict, rf.predict_proba)\n",
    "    mim_infl = explainer.explain_instance()\n",
    "    predict_proba = rf.predict_proba(data[-1:])[0]\n",
    "    predict_label = rf.predict(data[-1:])[0]\n",
    "\n",
    "    exp, feature2mim = explain_text_with_mim(mim=mim_infl, text_instance=newsgroups_test.data[idx], \n",
    "                                            tf_idf_instance=test_vectors[idx],\n",
    "                                            predict_proba=predict_proba, predict_label=predict_label,\n",
    "                                            word_to_id=word_to_id, num_features=10)\n",
    "\n",
    "    print('Document id: %d' % idx)\n",
    "    exp.show_in_notebook(text=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Keras",
   "language": "python",
   "name": "keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
